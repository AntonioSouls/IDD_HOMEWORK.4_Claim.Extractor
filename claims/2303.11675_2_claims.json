{
    "caption": "Table 2: Performance comparison on the AGORA dataset. Here, all the comparing methods adopt weak perspective projection and also fine-tune the model on the AGORA training set (except PARE).",
    "table": [
        "<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tr id=\"S4.T2.2.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.2.3.1.1\" class=\"ltx_text\">Method</span></td>&#13;\n<td id=\"S4.T2.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Agora</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center\">MJE<math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><ci id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center\">V2V<math id=\"S4.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.m1.1b\"><ci id=\"S4.T2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">SPIN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">153.4</td>&#13;\n<td id=\"S4.T2.2.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">148.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.5\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.5.1\" class=\"ltx_td ltx_align_left\">PARE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.5.2\" class=\"ltx_td ltx_align_center\">146.2</td>&#13;\n<td id=\"S4.T2.2.5.3\" class=\"ltx_td ltx_align_center\">140.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.6\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.6.1\" class=\"ltx_td ltx_align_left\">ROMP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.6.2\" class=\"ltx_td ltx_align_center\">108.1</td>&#13;\n<td id=\"S4.T2.2.6.3\" class=\"ltx_td ltx_align_center\">103.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.7\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.7.1\" class=\"ltx_td ltx_align_left\">BEV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.7.2\" class=\"ltx_td ltx_align_center\">105.3</td>&#13;\n<td id=\"S4.T2.2.7.3\" class=\"ltx_td ltx_align_center\">100.7</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.8\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.8.1\" class=\"ltx_td ltx_align_left\">Hand4Whole&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.8.2\" class=\"ltx_td ltx_align_center\">89.8</td>&#13;\n<td id=\"S4.T2.2.8.3\" class=\"ltx_td ltx_align_center\">84.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.9\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.9.1\" class=\"ltx_td ltx_align_left\">CLIFF&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.9.2\" class=\"ltx_td ltx_align_center\">81.0</td>&#13;\n<td id=\"S4.T2.2.9.3\" class=\"ltx_td ltx_align_center\">76.0</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.10\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">BoPR</td>&#13;\n<td id=\"S4.T2.2.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T2.2.10.2.1\" class=\"ltx_text ltx_font_bold\">79.9</span></td>&#13;\n<td id=\"S4.T2.2.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T2.2.10.3.1\" class=\"ltx_text ltx_font_bold\">74.5</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
    ],
    "footnotes": [
        "[18] [18]\r\n\r\nNikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis.\r\n\r\n\r\nLearning to reconstruct 3d human pose and shape via model-fitting in\r\nthe loop.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF International Conference on\r\nComputer Vision, pages 2252–2261, 2019.",
        "[16] [16]\r\n\r\nMuhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges, and Michael J Black.\r\n\r\n\r\nPare: Part attention regressor for 3d human body estimation.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF International Conference on\r\nComputer Vision, pages 11127–11137, 2021.",
        "[34] [34]\r\n\r\nYu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and Tao Mei.\r\n\r\n\r\nMonocular, one-stage, regression of multiple 3d people.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF International Conference on\r\nComputer Vision, pages 11179–11188, 2021.",
        "[35] [35]\r\n\r\nYu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J Black.\r\n\r\n\r\nPutting people in their place: Monocular regression of 3d people in\r\ndepth.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition, pages 13243–13252, 2022.",
        "[35] [35]\r\n\r\nYu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J Black.\r\n\r\n\r\nPutting people in their place: Monocular regression of 3d people in\r\ndepth.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition, pages 13243–13252, 2022.",
        "[20] [20]\r\n\r\nZhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan.\r\n\r\n\r\nCliff: Carrying location information in full frames into human pose\r\nand shape estimation.\r\n\r\n\r\nIn European Conference on Computer Vision, pages 590–606.\r\nSpringer, 2022."
    ],
    "references": [
        "We also evaluate our method on a multi-person dataset AGORA and compare it with state-of-the-art methods. Furthermore, since camera parameter estimation plays a crucial role in the performance of the AGORA dataset, we compare our method with those using weak-perspective projection in training for a fair comparison. Table 2 shows the result where our method performs better than multi-person-based approaches and previous state-of-the-art method CLIFF."
    ]
}