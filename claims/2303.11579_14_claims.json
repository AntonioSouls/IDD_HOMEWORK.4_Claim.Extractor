{
    "caption": "Table 7: Performance using other 3D estimators as backbone. MPJPE↓↓\\downarrow is reported. H=K=1.",
    "table": [
        "<table id=\"A4.T7.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"A4.T7.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"A4.T7.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A4.T7.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Backbone</span></td>&#13;\n<td id=\"A4.T7.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">&#13;\n<span id=\"A4.T7.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">P-STMO&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T7.3.1.1.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a><span id=\"A4.T7.3.1.1.2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>&#13;\n</td>&#13;\n<td id=\"A4.T7.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">&#13;\n<span id=\"A4.T7.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">PoseFormer&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T7.3.1.1.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">70</a><span id=\"A4.T7.3.1.1.3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>&#13;\n</td>&#13;\n<td id=\"A4.T7.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">&#13;\n<span id=\"A4.T7.3.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">STE&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T7.3.1.1.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a><span id=\"A4.T7.3.1.1.4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>&#13;\n</td>&#13;\n<td id=\"A4.T7.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">&#13;\n<span id=\"A4.T7.3.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">TCN&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T7.3.1.1.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a><span id=\"A4.T7.3.1.1.5.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"A4.T7.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"A4.T7.3.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A4.T7.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">w/o diffusion</span></td>&#13;\n<td id=\"A4.T7.3.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T7.3.1.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.7</span></td>&#13;\n<td id=\"A4.T7.3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T7.3.1.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">47.6</span></td>&#13;\n<td id=\"A4.T7.3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T7.3.1.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.6</span></td>&#13;\n<td id=\"A4.T7.3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T7.3.1.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">46.4</span></td>&#13;\n</tr>&#13;\n<tr id=\"A4.T7.3.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"A4.T7.3.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"A4.T7.3.1.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">w/ diffusion</span></td>&#13;\n<td id=\"A4.T7.3.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T7.3.1.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">42.2</span></td>&#13;\n<td id=\"A4.T7.3.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T7.3.1.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">45.7</span></td>&#13;\n<td id=\"A4.T7.3.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T7.3.1.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.8</span></td>&#13;\n<td id=\"A4.T7.3.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T7.3.1.3.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">44.5</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
    ],
    "footnotes": [
        "[51] [51]\r\n\r\nWenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao.\r\n\r\n\r\nP-stmo: Pre-trained spatial temporal many-to-one model for 3d human\r\npose estimation.\r\n\r\n\r\nIn Computer Vision–ECCV 2022: 17th European Conference, Tel\r\nAviv, Israel, October 23–27, 2022, Proceedings, Part V, pages 461–478.\r\nSpringer, 2022.",
        "[70] [70]\r\n\r\nCe Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming\r\nDing.\r\n\r\n\r\n3d human pose estimation with spatial and temporal transformers.\r\n\r\n\r\narXiv preprint arXiv:2103.10455, 2021.",
        "[35] [35]\r\n\r\nWenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang.\r\n\r\n\r\nExploiting temporal contexts with strided transformer for 3d human\r\npose estimation.\r\n\r\n\r\nIEEE Transactions on Multimedia, 2022.",
        "[46] [46]\r\n\r\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.\r\n\r\n\r\n3d human pose estimation in video with temporal convolutions and\r\nsemi-supervised training.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition, pages 7753–7762, 2019."
    ],
    "references": []
}